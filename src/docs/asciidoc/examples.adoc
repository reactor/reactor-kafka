== Sample Scenarios

This section shows sample code segments for typical scenarios where Reactor Kafka API
may be used. Full code listing for these scenarios are included in the
https://github.com/reactor/reactor-kafka/tree/main/reactor-kafka-samples[samples sub-project].

[[sample-producer]]
=== Sending records to Kafka

See <<api-guide-sender,KafkaSender API>> for details on the KafkaSender API for sending outbound records
to Kafka. The following code segment creates a simple pipeline that sends records to Kafka and
processes the responses. The outbound flow is triggered when the returned Flux is subscribed to.

[source,java]
--------
KafkaSender.create(SenderOptions.<Integer, String>create(producerProps).maxInFlight(512))   // <1>
           .send(outbound.map(r -> senderRecord(r)))                                        // <2>
           .doOnNext(result -> processResponse(result))                                     // <3>
           .doOnError(e -> processError(e));
--------

<1> Create a sender with maximum 512 messages in-flight
<2> Send a sequence of sender records
<3> Process send result when onNext is triggered

[[sample-consumer]]
=== Replaying records from Kafka topics

See <<api-guide-receiver,KafkaReceiver API>> for details on the KafkaReceiver API for consuming records
from Kafka topics. The following code segment creates a Flux that replays all records on a topic
and commits offsets after processing the messages. Manual acknowledgement provides
at-least-once delivery semantics.


[source,java]
--------
ReceiverOptions<Integer, String> options =
    ReceiverOptions.<Integer, String>create(consumerProps)
                   .consumerProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest")  // <1>
                   .commitBatchSize(10)                                                    // <2>
                   .subscription(Collections.singleton("demo-topic"));                     // <3>
KafkaReceiver.create(options)
             .receive()
             .doOnNext(r -> {
                     processRecord(r);                   // <4>
                     r.receiverOffset().acknowledge();   // <5>
                 })
             .subscribe();
--------
<1> Start consuming from first available offset on each partition if committed offsets are not available
<2> Commit every 10 acknowledged messages
<3> Topics to consume from
<4> Process consumer record from Kafka
<5> Acknowledge that record has been consumed


[[kafka-sink]]
=== Reactive pipeline with Kafka sink

The code segment below consumes messages from an external source, performs some transformation
and stores the output records in Kafka. Large number of retry attempts are configured
on the Kafka producer so that transient failures don't impact the pipeline. Source commits are
performed only after records are successfully written to Kafka.

[source,java]
--------
senderOptions = senderOptions
    .producerProperty(ProducerConfig.ACKS_CONFIG, "all")                  // <1>
    .producerProperty(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE)   // <2>
    .maxInFlight(128);                                                    // <3>
KafkaSender.create(senderOptions)
           .send(source.flux().map(r -> transform(r)))                      // <4>
           .doOnError(e-> log.error("Send failed, terminating.", e))        // <5>
           .doOnNext(r -> source.commit(r.correlationMetadata()));          // <6>
--------
<1> Send is acknowledged by Kafka for acks=all after message is delivered to all in-sync replicas
<2> Large number of retries in the producer to cope with transient failures in brokers
<3> Low in-flight count to avoid filling up producer buffer and blocking the pipeline, default stopOnError=true
<4> Receive from external source, transform and send to Kafka
<5> If a send fails, it indicates catastrophic error, fail the whole pipeline
<6> Use correlation metadata in the sender record to commit source record


[[kafka-source]]
=== Reactive pipeline with Kafka source

The code segment below consumes records from Kafka topics, transforms the record
and sends the output to an external sink. Kafka consumer offsets are committed after
records are successfully output to sink.

[source,java]
--------
receiverOptions = receiverOptions
    .commitInterval(Duration.ZERO)              // <1>
    .commitBatchSize(0)                         // <2>
    .subscription(Pattern.compile(topics));     // <3>
KafkaReceiver.create(receiverOptions)
             .receive()
             .publishOn(aBoundedElasticScheduler) // <4>
             .concatMap(m -> sink.store(transform(m))                                   // <5>
                               .doOnSuccess(r -> m.receiverOffset().commit().block())); // <6>
--------
<1> Disable periodic commits
<2> Disable commits by batch size
<3> Wildcard subscription
<4> Cannot block the receiver thread
<5> Tranform Kafka record and store in external sink
<6> Synchronous commit after record is successfully delivered to sink

[[kafka-source-sink]]
=== Reactive pipeline with Kafka source and sink

The code segment below consumes messages from Kafka topic, performs some transformation
on the incoming messages and stores the result in some Kafka topics. Manual acknowledgement
mode provides at-least-once semantics with messages acknowledged after the output records
are delivered to Kafka. Acknowledged offsets are committed periodically based on the
configured commit interval.

[source,java]
--------
receiverOptions = receiverOptions
    .commitInterval(Duration.ofSeconds(10))        // <1>
    .subscription(Pattern.compile(topics));
sender.send(KafkaReceiver.create(receiverOptions)
                         .receive()
                         .map(m -> SenderRecord.create(transform(m.value()), m.receiverOffset())))  // <2>
      .doOnNext(m -> m.correlationMetadata().acknowledge());  // <3>
--------
<1> Configure interval for automatic commits
<2> Transform incoming record and create outbound record with transformed data in the payload and inbound offset as correlation metadata
<3> Acknowledge the inbound offset using the offset instance in correlation metadata after outbound record is delivered to Kafka

[[at-most-once]]
=== At-most-once delivery

The code segment below demonstrates a flow with at-most once delivery. Producer does not wait for acks and
does not perform any retries. Messages that cannot be delivered to Kafka on the first attempt
are dropped. `KafkaReceiver` commits offsets before delivery to the application to ensure that if the consumer
restarts, messages are not redelivered. With replication factor 1 for topic partitions,
this code can be used for at-most-once delivery.

[source,java]
--------
senderOptions = senderOptions
    .producerProperty(ProducerConfig.ACKS_CONFIG, "0")     // <1>
    .producerProperty(ProducerConfig.RETRIES_CONFIG, "0")  // <2>
    .stopOnError(false);                                   // <3>
receiverOptions = receiverOptions
    .subscription(Collections.singleton(sourceTopic));
KafkaSender.create(senderOptions)
            .send(KafkaReceiver.create(receiverOptions)
                               .receiveAtmostOnce()                   // <4>
                               .map(cr -> SenderRecord.create(transform(cr.value()), cr.offset())));
--------
<1> Send with acks=0 completes when message is buffered locally, before it is delivered to Kafka broker
<2> No retries in producer
<3> Ignore any error and continue to send remaining records
<4> At-most-once receive

[[fan-out]]
=== Fan-out with Multiple Streams

The code segment below demonstrates fan-out with the same records processed in multiple independent
streams. Each stream is processed on a different thread and which transforms the input record
and stores the output in a Kafka topic.

Reactor's https://projectreactor.io/docs/core/release/api/reactor/core/publisher/EmitterProcessor.html[EmitterProcessor]
is used to broadcast the input records from Kafka to multiple subscribers.

[source,java]
--------

EmitterProcessor<Person> processor = EmitterProcessor.create();         // <1>
BlockingSink<Person> incoming = processor.connectSink();                // <2>
inputRecords = KafkaReceiver.create(receiverOptions)
                            .receive()
                            .doOnNext(m -> incoming.emit(m.value()));   // <3>

outputRecords1 = processor.publishOn(scheduler1).map(p -> process1(p)); // <4>
outputRecords2 = processor.publishOn(scheduler2).map(p -> process2(p)); // <5>

Flux.merge(sender.send(outputRecords1), sender.send(outputRecords2))
    .doOnSubscribe(s -> inputRecords.subscribe())
    .subscribe();                                                       // <6>
--------
<1> Create publish/subscribe EmitterProcessor for fan-out of Kafka inbound records
<2> Create BlockingSink to which records are emitted
<3> Receive from Kafka and emit to BlockingSink
<4> Consume records on a scheduler, process and generate output records to send to Kafka
<5> Add another processor for the same input data on a different scheduler
<6> Merge the streams and subscribe to start the flow


[[concurrent-ordered]]
=== Concurrent Processing with Partition-Based Ordering

The code segment below demonstrates a flow where messages are consumed from a Kafka topic, processed
by multiple threads and the results stored in another Kafka topic. Messages are grouped
by partition to guarantee ordering in message processing and commit operations. Messages
from each partition are processed on a single thread.

[source,java]
--------

Scheduler scheduler = Schedulers.newElastic("sample", 60, true);
KafkaReceiver.create(receiverOptions)
             .receive()
             .groupBy(m -> m.receiverOffset().topicPartition())                  // <1>
             .flatMap(partitionFlux ->
                 partitionFlux.publishOn(scheduler)
                              .map(r -> processRecord(partitionFlux.key(), r))
                              .sample(Duration.ofMillis(5000))                   // <2>
                              .concatMap(offset -> offset.commit()));            // <3>
--------
<1> Group by partition to guarantee ordering
<2> Commit periodically
<3> Commit in sequence using concatMap

[[transactional-sender]]
=== Transactional send

The code segment below consumes messages from an external source, performs some transformation
and stores multiple transformed records in different Kafka topics within a transaction.

[source,java]
--------
senderOptions = senderOptions
    .producerProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "SampleTxn");       // <1>
KafkaSender.create(senderOptions)
           .sendTransactionally(source.map(r -> Flux.fromIterable(transform(r)))) // <2>
           .concatMap(r -> r)
           .doOnError(e-> log.error("Send failed, terminating.", e))
           .doOnNext(r -> log.debug("Send completed {}", r.correlationMetadata());
--------
<1> Configure transactional id for producer
<2> Send multiple records generated from each source record within a transaction

[[exactly-once]]
=== Exactly-once delivery

The code segment below demonstrates a flow with exactly once delivery. Source records
received from a Kafka topic are transformed and sent to Kafka. Each batch of records
is delivered to the application in a new transaction. Offsets of the source records
of each batch are automatically committed within its transaction. Each transaction
is committed by the application after the transformed records of the batch are
successfully delivered to the destination topic. Next batch of records is delivered
to the application in a new transaction after the current transaction is committed.

[source,java]
--------
senderOptions = senderOptions
    .producerProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "SampleTxn");    // <1>
receiverOptions = receiverOptions
    .consumerProperty(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed") // <2>
    .subscription(Collections.singleton(sourceTopic));
sender = KafkaSender.create(senderOptions);
transactionManager = sender.transactionManager();
receiver.receiveExactlyOnce(transactionManager)                                // <3>
        .concatMap(f -> sender.send(f.map(r -> transform(r)))                  // <4>
                              .concatWith(transactionManager.commit()))        // <5>
        .onErrorResume(e -> transactionManager.abort().then(Mono.error(e)))    // <6>

--------
<1> Configure transactional id for producer
<2> Consume only committed messages
<3> Receive exactly once within transactions, offsets are auto-committed when transaction is committed
<4> Send transformed records within the same transaction as source record offsets
<5> Commit transaction after sends complete successfully
<6> Abort transaction if send fails and propagate error
