== Reactor Kafka API

[[api-guide-overview]]
=== Overview

This section describes the reactive API for producing and consuming messages using Apache Kafka.
There are two main interfaces in Reactor Kafka:

. `reactor.kafka.KafkaSender` for publishing messages to Kafka
. `reactor.kafka.KafkaReceiver` for consuming messages from Kafka

Full API for Reactor Kafka is available in the link:../../api/index.html[javadocs].

The project uses https://github.com/reactor/reactor-core[Reactor Core] to expose a https://github.com/reactive-streams/reactive-streams-jvm["Reactive Streams"] API.


[[api-guide-sender]]
=== Reactive Kafka Sender

Outbound messages are sent to Kafka using `reactor.kafka.KafkaSender`. Senders are thread-safe and can be shared
across multiple threads to improve throughput. A `KafkaSender` is associated with one `KafkaProducer` that is used
to transport messages to Kafka.

A KafkaSender is created with an instance of sender configuration options `reactor.kafka.sender.SenderOptions`.
Changes made to `SenderOptions` after the creation of `KafkaSender` will not be used by the KafkaSender.
The properties of SenderOptions such as a list of bootstrap Kafka brokers and serializers are passed down
to the underlying `KafkaProducer`. The properties may be configured on the SenderOptions instance at creation time
or by using the setter `SenderOptions#producerProperty`. Other configuration options for the reactive KafkaSender like
the maximum number of in-flight messages can also be configured before the KafkaSender instance is created.

The generic types of `SenderOptions<K, V>` and `KafkaSender<K, V>` are the key and value types of producer records
published using the KafkaSender and corresponding serializers must be set on the SenderOptions instance before
the KafkaSender is created.


[source,java]
--------
Map<String, Object> producerProps = new HashMap<>();
producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);
producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

SenderOptions<Integer, String> senderOptions =
    SenderOptions.<Integer, String>create(producerProps)       // <1>
                 .maxInFlight(1024);                           // <2>
--------
<1> Specify properties for underlying `KafkaProducer`
<2> Configure options for reactive KafkaSender

Once the required options have been configured on the options instance, a new `KafkaSender` instance
can be created with the options already configured in `senderOptions`.

[source,java]
--------
KafkaSender<Integer, String> sender = KafkaSender.create(senderOptions);
--------

The KafkaSender is now ready to send messages to Kafka.
The underlying `KafkaProducer` instance is created lazily when the first message is ready to be sent.
At this point, a `KafkaSender` instance has been created, but no connections to Kafka have been made yet.

Let's now create a sequence of messages to send to Kafka. Each outbound message to be sent to Kafka
is represented as a `SenderRecord`.  A `SenderRecord` is a Kafka
https://kafka.apache.org/0102/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[ProducerRecord]
with additional correlation metadata for matching send results to records. `ProducerRecord` consists of a key/value pair
to send to Kafka and the name of the Kafka topic to send the message to. Producer records may also optionally
specify a partition to send the message to or use the configured partitioner to choose a partition. Timestamp may
also be optionally specified in the record and if not specified, the current timestamp will be assigned by the Producer.
The additional correlation metadata included in `SenderRecord` is not sent to Kafka, but is included in the
`SendResult` generated for the record when the send operation completes or fails. Since results of sends to
different partitions may be interleaved, the correlation metadata enables results to be matched to their corresponding record.

A https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html[Flux<SenderRecord>] of records
is created for sending to Kafka. For beginners, https://github.com/reactor/lite-rx-api-hands-on[Lite Rx API Hands-on]
provides a hands-on tutorial on using the Reactor classes `Flux` and `Mono`.


[source,java]
--------
Flux<SenderRecord<Integer. String> outboundFlux =
    Flux.range(1, 10)
        .map(i -> SenderRecord.create(topic, partition, timestamp, i, "Message_" + i, i);
--------

The code segment above creates a sequence of messages to send to Kafka, using the message index as
correlation metadata in each `SenderRecord`. The outbound Flux can now be sent to Kafka using the
`KafkaSender` created earlier.

The code segment below sends the records to Kafka and prints out the response metadata received from Kafka
and the correlation metadata for each record.  The final `subscribe()` in the code block
requests upstream to send the records to Kafka and the response metadata received from Kafka flow downstream.
As each result is received, the record metadata from Kafka along with the correlation metadata identifying the
record is printed out to console by the `onNext` handler. The response from Kafka includes the partition to which
the record was sent as well as the offset at the which the record was appended, if available.
When records are sent to multiple partitions, responses arrive in order
for each partition, but responses from different partitions may be interleaved.

[source,java]
--------
sender.send(outboundFlux)                          // <1>
      .doOnError(e-> log.error("Send failed", e))  // <2>
      .doOnNext(r -> System.out.printf("Message #%d send response: %s\n", r.correlationMetadata(), r.recordMetadata())) <3>
      .subscribe();    // <4>
--------
<1> Reactive send operation for the outbound Flux
<2> If Kafka send fails, log an error
<3> Print metadata returned by Kafka and the message index in `correlationMetadata()`
<4> Subscribe to trigger the actual flow of records from `outboundFlux` to Kafka.


See https://github.com/reactor/reactor-kafka/blob/master/reactor-kafka-samples/src/main/java/reactor/kafka/samples/SampleProducer.java  for the full code listing of a sample producer.

==== Error handling

[source,java]
--------
public SenderOptions<K, V> stopOnError(boolean stopOnError);
--------

`SenderOptions#stopOnError()` specifies whether each send sequence should fail immediately if one
record could not delivered to Kafka after the configured number of retries or wait until all records
have been processed. This can be used along with `ProducerConfig#ACKS_CONFIG`
and `ProducerConfig#RETRIES_CONFIG` to configure the required quality of service.

[source,java]
--------
<T> Flux<SenderResult<T>> send(Publisher<SenderRecord<K, V, T>> outboundRecords);
--------

If `stopOnError` is false, a success or error response is returned for each outgoing record.
For error responses, the exception from Kafka indicating the reason for send failure is set on `SenderResult`
and can be retrieved using `SenderResult#exception()`. The Flux fails with an error after attempting to send
all records published on `outboundRecords`. If `outboundRecords` is a non-terminating `Flux`, send continues to send
records published on this `Flux` until the result `Flux` is explicitly cancelled by the user.

If `stopOnError` is true, a response is returned for the first failed send and the result Flux is terminated
immediately with an error. Since multiple outbound messages may be in-flight at any time, it is possible that
some messages are delivered successfully to Kafka after the first failure is detected. `SenderOptions#maxInFlight()`
option may be configured to limit the number of messages in-flight at any time.

==== Send without result metadata

If individual results are not required for each send request, `ProducerRecord` can be sent to Kafka
without providing correlation metadata using the `sendOutbound` method. `KafkaOutbound` is a fluent
interface that enables sends to be chained together.

[source,java]
--------
KafkaOutbound<K, V> sendOutbound(Publisher<? extends ProducerRecord<K, V>> records);
--------

The send sequence is initiated by subscribing to the Mono obtained from `KafkaOutbound#then()`.
The returned Mono completes successfully if all the outbound records are delivered successfully. The Mono
terminates on the first send failure. If `outboundRecords` is a non-terminating Flux, records continue to
be sent to Kafka unless a send fails or the returned Mono is cancelled.

[source,java]
--------
sender.sendOutbound(Flux.range(1,  10)
                        .map(i -> new ProducerRecord<Integer, String>(topic, i, "Message_" + i))) // <1>
      .then()                                                    // <2>
      .doOnError(e -> e.printStackTrace())                       // <3>
      .doOnSuccess(s -> System.out.println("Sends succeeded"))   // <4>
      .subscribe();                                              // <5>
--------
<1> Create `ProducerRecord` Flux. Records are not wrapped in `SenderRecord`
<2> Get the `Mono` to subscribe to for starting the message flow
<3> Error indicates failure to send one or more records
<4> Success indicates all records were published, individual partitions or offsets not returned
<5> Subscribe to request the actual sends

Multiple sends can be chained together using a sequence of sends on the returned `KafkaOutbound`.
When the Mono returned from `KafkaOutbound#then()` is subscribed to, the sends are invoked
in sequence in the declaration order. The sequence is cancelled if any of the sends fail
after the configured number of retries.

[source,java]
--------
sender.sendOutbound(flux1)                                       // <1>
      .send(flux2)
      .send(flux3))
      .then()                                                    // <2>
      .doOnError(e -> e.printStackTrace())                       // <3>
      .doOnSuccess(s -> System.out.println("Sends succeeded"))   // <4>
      .subscribe();                                              // <5>
--------
<1> Sends `flux1`, `flux2` and `flux3` in order
<2> Get the `Mono` to subscribe to for starting the message flow sequence
<3> Error indicates failure to send one or more records from any of the sends in the chain
<4> Success indicates successful send of all records from the whole chain
<5> Subscribe to initiate the sequence of sends in the chain


Note that in all cases the retries configured for the `KafkaProducer` are attempted and failures returned by
the reactive `KafkaSender` indicate a failure to send after the configured number of retry attempts. Retries
can result in messages being delivered out of order. The producer property
`ProducerConfig#MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION` may be set to one to avoid re-ordering.

==== Threading model

`KafkaProducer` uses a separate network thread for sending requests and processing responses. To ensure
that the producer network thread is never blocked by applications while processing results, `KafkaSender`
delivers responses to applications on a separate scheduler. By default, this is a single threaded
pooled scheduler that is freed when no longer required. The scheduler can be overridden if required, for instance,
to use a parallel scheduler when the Kafka sends are part of a larger pipeline. This is done on the `SenderOptions`
instance before the KafkaSender instance is created using:


[source,java]
--------
public SenderOptions<K, V> scheduler(Scheduler scheduler);
--------

==== Non-blocking back-pressure

The number of in-flight sends can be controlled using the `maxInFlight` option. Requests for more elements from
upstream are limited by the configured `maxInFlight` to ensure that the total number of requests at any time for which
responses are pending are limited. Along with `buffer.memory` and `max.block.ms` options on `KafkaProducer`,
`maxInFlight` enables control of memory and thread usage when `KafkaSender` is used in a reactive pipeline. This option
can be configured on `SenderOptions` before the KafkaSender is created. Default value is 256. For small messages,
 a higher value will improve throughput.


[source,java]
--------
public SenderOptions<K, V> maxInFlight(int maxInFlight);
--------

==== Closing the KafkaSender

When the KafkaSender is no longer required, the KafkaSender instance can be closed. The underlying `KafkaProducer` is closed,
closing all client connections and freeing all memory used by the producer.

[source,java]
--------
sender.close();
--------

==== Access to the underlying `KafkaProducer`

Reactive applications may sometimes require access to the underlying producer instance to perform actions that are not
exposed by the `KafkaSender` interface. For example, an application might need to know the number of partitions in a topic
in order to choose the partition to send a record to. Operations that are not provided directly by `KafkaSender` like `send`
can be run on the underlying `KafkaProducer` using `KafkaSender#doOnProducer`.

[source,java]
--------
sender.doOnProducer(producer -> producer.partitionsFor(topic))
      .doOnSuccess(partitions -> System.out.println("Partitions " + partitions))
      .subscribe();
--------

User provided methods are executed asynchronously.
A `Mono` is returned by `doOnProducer` which completes with the value returned by the user-provided function.


[[api-guide-receiver]]
=== Reactive Kafka Receiver

Messages stored in Kafka topics are consumed using the reactive receiver `reactor.kafka.receiver.KafkaReceiver`.
Each instance of `KafkaReceiver` is associated with a single instance of `KafkaConsumer`. `KafkaReceiver` is not thread-safe
since the underlying `KafkaConsumer` cannot be accessed concurrently by multiple threads.

A receiver is created with an instance of receiver configuration options `reactor.kafka.receiver.ReceiverOptions`.
Changes made to `ReceiverOptions` after the creation of the receiver instance will not be used by the `KafkaReceiver`.
The properties of ReceiverOptions such as a list of bootstrap Kafka brokers and de-serializers are passed down
to the underlying `KafkaConsumer`. These properties may be configured on the ReceiverOptions instance at creation time
or by using the setter `ReceiverOptions#consumerProperty`. Other configuration options for the reactive
KafkaReceiver including subscription topics must be added to options before the KafkaReceiver instance is created.

The generic types of `ReceiverOptions<K, V>` and `KafkaReceiver<K, V>` are the key and value types of consumer records
consumed using the receiver and corresponding de-serializers must be set on the ReceiverOptions instance before
the KafkaReceiver is created.

[source,java]
--------
Map<String, Object> consumerProps = new HashMap<>();
consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, "sample-group");
consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);
consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

ReceiverOptions<Integer, String> receiverOptions =
    ReceiverOptions.<Integer, String>create(consumerProps)         // <1>
                   .subscription(Collections.singleton(topic));    // <2>
--------
<1> Specify properties to be provided to `KafkaConsumer`
<2> Topics to subscribe to

Once the required configuration options have been configured on the options instance, a new `KafkaReceiver` instance
can be created with these options to consume inbound messages.
The code block below creates a receiver instance and creates an inbound Flux for the receiver.
The underlying `KafkaConsumer` instance is created lazily later when the inbound Flux is subscribed to.


[source,java]
--------
Flux<ReceiverRecord<Integer, String>> inboundFlux =
    KafkaReceiver.create(receiverOptions)
                 .receive();
--------

The inbound Kafka Flux is ready to be consumed. Each inbound message delivered by the Flux is represented
as a `ReceiverRecord`. Each receiver record is a
https://kafka.apache.org/0102/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord]
returned by `KafkaConsumer` along with a committable `ReceiverOffset` instance. The offset must be acknowledged
after the message is processed since unacknowledged offsets will not be committed.
If commit interval or commit batch size are configured, acknowledged offsets will be committed periodically.
Offsets may also be committed manually using `ReceiverOffset#commit()` if finer grained control of commit
operations is required.



[source,java]
--------
inboundFlux.subscribe(r -> {
    System.out.printf("Received message: %s\n", r);           // <1>
    r.receiverOffset().acknowledge();                         // <2>
});
--------
<1> Prints each consumer record from Kafka
<2> Acknowledges that the record has been processed so that the offset may be committed

==== Subscribing to wildcard patterns

The example above subscribed to a single Kafka topic. The same API can be used to subscribe to
more than one topic by specifying multiple topics in the collection provided to `ReceiverOptions#subscription()`.
Subscription can also be made to a wildcard pattern by specifying a pattern to subscribe to. Group
management in `KafkaConsumer` dynamically updates topic assignment when topics matching the pattern
are created or deleted and assigns partitions of matching topics to available consumer instances.

[source,java]
--------
receiverOptions = receiverOptions.subscription(Pattern.compile("demo.*"));  // <1>
--------
<1> Consume records from all topics starting with "demo"

Changes to `ReceiverOptions` must be made before the receiver instance is created. Altering the subscription
deletes any existing subscriptions on the options instance.

==== Manual assignment of topic partitions

Partitions may be manually assigned to the receiver without using Kafka consumer group management.

[source,java]
--------
receiverOptions = receiverOptions.assignment(Collections.singleton(new TopicPartition(topic, 0)); // <1>
--------
<1> Consume from partition 0 of specified topic

Existing subscriptions and assignments on the options instance are deleted when a new assignment
is specified. Every receiver created from this options instance with manual assignment consumes messages
from all the specified partitions.

==== Controlling commit frequency

Commit frequency can be controlled using a combination of commit interval
and commit batch size. Commits are performed when either the interval or batch size is reached. One or both
of these options may be set on `ReceiverOptions` before the receiver instance is created. If commit interval
is configured, at least one commit is scheduled within that interval if any records were
consumed. If commit batch size is configured, a commit is scheduled when the configured number of records
are consumed and acknowledged.

Manual acknowledgement of consumed records after processing along with automatic commits based on
the configured commit frequency provides at-least-once delivery semantics. Messages are re-delivered
if the consuming application crashes after message was dispatched but before it was processed and
acknowledged. Only offsets explicitly acknowledged using `ReceiverOffset#acknowledge()` are committed.
Note that acknowledging an offset acknowledges all previous offsets on the same partition. All
acknowledged offsets are committed when partitions are revoked during rebalance and when the receive
Flux is terminated.

Applications which require fine-grained control over the timing of commit operations
can disable periodic commits and explicitly invoke `ReceiverOffset#commit()` when required to trigger
a commit. This commit is asynchronous by default, but the application many invoke `Mono#block()`
on the returned Mono to implement synchronous commits. Applications may batch commits by acknowledging
messages as they are consumed and invoking commit() periodically to commit acknowledged offsets.

[source,java]
--------
receiver.receive()
        .doOnNext(r -> {
                process(r);
                r.receiverOffset().commit().block();
            });
--------

Note that committing an offset acknowledges and commits all previous offsets on that partition. All
acknowledged offsets are committed when partitions are revoked during rebalance and when the receive
Flux is terminated.

==== Auto-acknowledgement of batches of records

`KafkaReceiver#receiveAutoAck` returns a `Flux` of batches of records returned by each `KafkaConsumer#poll()`.
The records in each batch are automatically acknowledged when the Flux corresponding to the batch terminates.

[source,java]
--------
KafkaReceiver.create(receiverOptions)
             .receiveAutoAck()
             .concatMap(r -> r)                                      // <1>
             .subscribe(r -> System.out.println("Received: " + r));  // <2>
--------
<1> Concatenate in order
<2> Print out each consumer record received, no explicit ack required

The maximum number of records in each batch can be controlled using the `KafkaConsumer` property
`MAX_POLL_RECORDS`. This is used together with the fetch size and wait times configured on the
KafkaConsumer to control the amount of data fetched from Kafka brokers in each poll. Each batch is
returned as a Flux that is acknowledged after the Flux terminates. Acknowledged records are committed periodically
based on the configured commit interval and batch size. This mode is simple to use since applications
do not need to perform any acknowledge or commit actions. It is efficient as well and can be used
for at-least-once delivery of messages.

==== Disabling automatic commits

Applications which don't require offset commits to Kafka may disable automatic commits by not acknowledging
any records consumed using `KafkaReceiver#receive()`.

[source,java]
--------
receiverOptions = ReceiverOptions.create()
        .commitInterval(Duration.ZERO)             // <1>
        .commitBatchSize(0);                       // <2>
KafkaReceiver.create(receiverOptions)
             .receive()
             .subscribe(r -> process(r));          // <3>
--------
<1> Disable periodic commits
<2> Disable commits based on batch size
<3> Process records, but don't acknowledge


==== At-most-once delivery
Applications may disable automatic commits to avoid re-delivery of records. `ConsumerConfig#AUTO_OFFSET_RESET_CONFIG`
can be configured to "latest" to consume only new records. But this could mean that an unpredictable
number of records are not consumed if an application fails and restarts.

`KafkaReceiver#receiveAtmostOnce` can be used to consume records with at-most-once semantics with a configurable
number of records-per-partition that may be lost if the application fails or crashes. Offsets are committed
synchronously before the corresponding record is dispatched. Records are guaranteed not to be re-delivered
even if the consuming application fails, but some records may not be processed if an application fails
after the commit before the records could be processed.

This mode is expensive since each record is committed individually and records are not delivered until
the commit operation succeeds. `ReceiverOptions#atmostOnceCommitCommitAheadSize` may be configured
to reduce the cost of commits and avoid blocking before dispatch if the offset of the record has already
been committed. By default, commit-ahead is disabled and at-most one record is lost per-partition if
an application crashes. If commit-ahead is configured, the maximum number of records that may be
lost per-partition is `ReceiverOptions#atmostOnceCommitCommitAheadSize + 1`.


[source,java]
--------
KafkaReceiver.create(receiverOptions)
             .receiveAtmostOnce()
             .subscribe(r -> System.out.println("Received: " + r));  // <1>
--------
<1> Process each consumer record, this record is not re-delivered if the processing fails

==== Partition assignment and revocation listeners

Applications can enable assignment and revocation listeners to perform any actions when
partitions are assigned or revoked from a consumer.

When group management is used, assignment listeners are invoked whenever partitions are assigned
to the consumer after a rebalance operation.  When manual assignment is used, assignment listeners
are invoked when the consumer is started. Assignment listeners can be used to seek to particular offsets
in the assigned partitions so that messages are consumed from the specified offset.

When group management is used, revocation listeners are invoked whenever partitions are revoked
from a consumer after a rebalance operation. When manual assignment is used, revocation listeners
are invoked before the consumer is closed. Revocation listeners can be used to commit processed
offsets when manual commits are used. Acknowledged offsets are automatically committed on revocation
if automatic commits are enabled.

==== Controlling start offsets for consuming records

By default, receivers start consuming records from the last committed offset of each assigned partition.
If a committed offset is not available, the offset reset strategy `ConsumerConfig#AUTO_OFFSET_RESET_CONFIG`
configured for the `KafkaConsumer` is used to set the start offset to the earliest or latest offset on the partition.
Applications can override offsets by seeking to new offsets in an assignment listener. Methods are provided on
`ReceiverPartition` to seek to the earliest, latest or a specific offset in the partition.


[source,java]
--------
void seekToBeginning();
void seekToEnd();
void seek(long offset);
--------

For example, the following code block starts consuming messages from the latest offset.


[source,java]
--------
receiverOptions = receiverOptions
            .addAssignListener(partitions -> partitions.forEach(p -> p.seekToEnd())) // <1>
            .subscription(Collections.singleton(topic));
KafkaReceiver.create(receiverOptions).receive().subscribe();
--------
<1> Seek to the last offset in each assigned partition


==== Consumer lifecycle

Each `KafkaReceiver` instance is associated with a `KafkaConsumer` that is created when the inbound
Flux returned by one of the receive methods in `KafkaReceiver` is subscribed to. The consumer is kept alive until
the Flux completes. When the Flux completes, all acknowledged offsets are committed and the
underlying consumer is closed. In Kafka version 0.10.0.x, heartbeats are sent by `KafkaConsumer`
only when applications invoke `KafkaConsumer#poll()`. Hence delays in processing messages can
result in session timeouts causing rebalance to be triggered. To avoid this, `KafkaReceiver` triggers
periodic heartbeats when application processing takes longer than the heartbeat interval for
older versions of Kafka. In Kafka version 0.10.1.0 and above, `KafkaConsumer` sends
heartbeats from a background thread to avoid this issue and `KafkaReceiver` does not track heartbeats.

Only one receive operation may be active in a `KafkaReceiver` at any one time. Any of the receive
methods can be invoked after the receive Flux corresponding to the last receive is terminated.


